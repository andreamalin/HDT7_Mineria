{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statsmodels.stats.diagnostic as diag\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, RepeatedKFold, cross_validate, cross_val_score\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, r2_score\n",
    "from scipy.stats import normaltest\n",
    "from sklearn import metrics\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score ,precision_score,recall_score,f1_score\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "seed = random.seed(123)\n",
    "number_clusters = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Exploratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable classification\n",
    "train_data = pd.read_csv('./data/train.csv', encoding = \"ISO-8859-1\")\n",
    "test_data = pd.read_csv('./data/test.csv', encoding = \"ISO-8859-1\")\n",
    "variables = pd.read_csv('./data/variables.txt', encoding = \"ISO-8859-1\")\n",
    "quant_vars = list(variables.loc[(variables['Clasification'] == 'Cuantitativa')]['Variable'].values)\n",
    "quali_vars = list(variables.loc[(variables['Clasification'] == 'Cualitativa')]['Variable'].values)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizando las variables numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[quant_vars].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in quant_vars:\n",
    "    data = train_data[var].dropna(how='all', axis=0)\n",
    "    \n",
    "    # GrÃ¡fico\n",
    "    sns.displot(data, kde=True)\n",
    "\n",
    "    # Mostrando normalidad\n",
    "    print('\\033[1m' + var + '\\033[0m' + ': Kurtosis:', stats.kurtosis(data), 'Skewness:', stats.skew(data), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizando las variables categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in quali_vars:\n",
    "  plt.figure(figsize=(20,5))\n",
    "  train_data[var].value_counts().plot(kind='bar')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizando la variable de interes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skewness and kurtosis\n",
    "print('Skewness: %f' % train_data['SalePrice'].skew())\n",
    "print('Kurtosis: %f' % train_data['SalePrice'].kurt())\n",
    "print('\\n---Describe---')\n",
    "train_data['SalePrice'].describe([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.65, 0.7, 0.8, 0.9, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat,p = stats.shapiro(train_data[[\"SalePrice\"]].dropna())\n",
    "print('Kolmogorov-Smirnov:\\np=%f\\n'% p)\n",
    "ks_statistic, p_value = diag.lilliefors(train_data[[\"SalePrice\"]].dropna())\n",
    "print('Lilliefors:\\nks=%f\\np=%f'%(ks_statistic,p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.displot(train_data['SalePrice'], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10 #number of variables for heatmap\n",
    "corrmat = train_data.corr()\n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(train_data[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obteniendo la relacion entre las variables mas significativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_data[cols],hue=\"SalePrice\")\n",
    "plt.show()\n",
    "# quant_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizando data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(row): \n",
    "    if row['SalePrice'] > 0 and row['SalePrice'] <= 179280:\n",
    "        return 'Low'\n",
    "    elif row['SalePrice'] > 179280 and row['SalePrice'] < 326100:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Expensive'\n",
    "   \n",
    "train_data['SalesCategories'] = train_data.apply(lambda row: categorize(row), axis=1)\n",
    "\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copied_train_data = train_data.copy()\n",
    "copied_train_data = copied_train_data.fillna(0)\n",
    "\n",
    "target = copied_train_data.pop('SalesCategories')\n",
    "data = copied_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricas = copied_train_data[quali_vars]\n",
    "numericas = copied_train_data[quant_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se transforman las colunas usando los preprocesadores\n",
    "numeric_preprocessor = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "# Se preparan los preprocesadores\n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "\n",
    "preprocesador = ColumnTransformer([\n",
    "    ('one_hot_encoder', categorical_preprocessor, quali_vars),\n",
    "    ('numerico', numeric_preprocessor, quant_vars)\n",
    "],remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70% de entrenamiento y 30% prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "  if (data[item].dtype == 'object'):\n",
    "    data = data.astype({item: str})\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target,test_size=0.3,train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = make_pipeline(preprocesador, SVC(kernel=\"linear\"))\n",
    "\n",
    "# Esto nos permite ver dentro del modelo\n",
    "set_config(display='diagram')\n",
    "modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados esperados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = modelo.predict(X_test)\n",
    "# print(target_pred)\n",
    "print (\"Accuracy:\",metrics.accuracy_score(y_test, X_pred))\n",
    "print (\"Precision:\", metrics.precision_score(y_test, X_pred,average='weighted') )\n",
    "print (\"Recall: \", metrics.recall_score(y_test, X_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(X_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscando automaticamente los mejores valores para el modelo polinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = make_pipeline(preprocesador, SVC(kernel=\"poly\"))\n",
    "modelo.fit(X_train,y_train)\n",
    "param_grid = {\n",
    "    'svc__C': (0.01, 0.1, 1, 4,16,32),\n",
    "    'svc__degree':(2, 3, 4)\n",
    "}\n",
    "model_grid_search = GridSearchCV(modelo, param_grid=param_grid, n_jobs=2, cv=10)\n",
    "model_grid_search.fit(X_train, y_train)\n",
    "accuracy = model_grid_search.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: \",accuracy)\n",
    "model_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscando automaticamente los mejores valores para el modelo polinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = make_pipeline(preprocesador, SVC(kernel=\"rbf\"))\n",
    "modelo.fit(X_train,y_train)\n",
    "param_grid = {\n",
    "    'svc__C': (0.01, 0.1, 1, 4,16,32),\n",
    "    'svc__gamma':(2e-5, 2e-3,0.01,0.1,4,16,32)\n",
    "    }\n",
    "    \n",
    "\n",
    "model_grid_search = GridSearchCV(modelo, param_grid=param_grid, n_jobs=2, cv=10)\n",
    "model_grid_search.fit(X_train, y_train)\n",
    "accuracy = model_grid_search.score(X_test, y_test)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "model_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform the target for official scoring\n",
    "train = train_data[quant_vars].dropna().copy()\n",
    "y = train.pop(\"SalePrice\") #La variable respuesta\n",
    "X = train #El resto de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,y_train, y_test = train_test_split(X, y,test_size=0.3,train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelosvr = SVR(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelosvr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon-Support Vector Regression.\n",
    "y_pred = modelosvr.predict(X_test)\n",
    "score = modelosvr.score(X_train, y_train)\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c33184972b48748d94ddd34e441bc25e543b9b1491b698e37ff4356c126c0090"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
